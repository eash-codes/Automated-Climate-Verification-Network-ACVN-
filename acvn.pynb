{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121144,"databundleVersionId":14484960,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸŒ Climate Misinformation Combat Agent\n\n## What's This Project About?\n\nClimate misinformation spreads like wildfire on social media. False claims about global warming, renewable energy, and climate science confuse people and slow down action. This project fights back.\n\nI built a smart AI system that automatically:\n- **Catches** false or misleading climate claims\n- **Verifies** them against real scientific sources (NASA, IPCC, NOAA)\n- **Explains** why they're wrong with evidence\n- **Creates** shareable counter-narratives for social media\n- **Generates** professional reports (PDF + JSON)\n\nThink of it as a fact-checking robot trained specifically for climate misinformation.\n\n---\n\n## The Problem\n\nEvery day, millions see posts like:\n- \"Climate change is a hoax invented by China\"\n- \"Solar panels pollute more than they help\"\n- \"Arctic ice is actually increasing\"\n- \"A few degrees of warming won't hurt\"\n\nThese claims sound plausible but they're **scientifically wrong**. The problem? There's no automated system to debunk them at scale across social media.\n\n---\n\n## The Solution: 5 AI Agents Working Together\n\nInstead of one big AI trying to do everything, I created 5 specialized agents. Each one is really good at one job:\n\n| Agent          |             Job        |                Example Output                |\n|----------------|------------------------|----------------------------------------------|\n| **Agent 1** ðŸ” | Detects climate claims | \"This is climate denial, needs verification\" |\n| **Agent 2** âœ“ | Verifies against sources | \"FALSE - contradicts NASA data\" |\n| **Agent 3** ðŸ“š | Writes fact-checks | \"Why this claim is wrong: ...\" |\n| **Agent 4** â­ | Scores credibility | \"23/100 - Highly unreliable\" |\n| **Agent 5** ðŸ“± | Makes social posts | \"ðŸš« Myth: ... REALITY: ... #FactCheck\" |\n\nEach agent passes its output to the next. The final result? A complete fact-check package ready to share.\n\n---\n\n## How It Works (The Flow)\n\n\n1. Raw Climate Claim (from news, Reddit, Twitter)\n2. Agent 1: Detect & Classify\n3. Agent 2: Verify Against Sources\n4. Agent 3: Synthesize Evidence\n5. Agent 4: Calculate Score\n6. Agent 5: Generate Social PostsProfessional PDF + JSON Report\n\n","metadata":{}},{"cell_type":"markdown","source":"## What Makes This Different?\n\nâœ… **Real-time** - Scrapes live climate news and analyzes it  \nâœ… **Multi-agent** - 5 specialized AIs working together (not one doing everything)  \nâœ… **Evidence-based** - Checks against NASA, IPCC, NOAA (not just opinions)  \nâœ… **Social-ready** - Creates posts for Twitter, Facebook, Instagram, LinkedIn, TikTok  \nâœ… **Professional reports** - Beautiful PDFs + structured JSON data  \nâœ… **No truncation** - Full, complete outputs (not summaries)  \n\n---\n\n## Who Should Care?\n\n- ðŸŒ **Fact-checkers** - Automate climate claim verification\n- ðŸ“± **Social media teams** - Get ready-to-post counter-narratives\n- ðŸŽ“ **Researchers** - Study patterns in climate misinformation\n- ðŸ›ï¸ **NGOs/Governments** - Scale fact-checking operations\n- ðŸ¤– **AI enthusiasts** - Learn multi-agent architecture\n\n---\n\n## What I Learnt From This Project\n\n- How to build a **multi-agent AI system**\n- How to use **Google Gemini API** for complex tasks\n- How to **orchestrate AI agents** (one feeding into another)\n- How to **scrape live data** and process it\n- How to **generate professional PDFs** from AI outputs\n- How to structure code for **production use**\n\n---\n\n## Quick Results Example\n\n**Input:** \"Arctic ice is increasing, not melting\"\n\n**Output:** \n\nVerdict: FALSE âŒ\nCredibility: 12/100 (Unreliable)\nScientific Consensus: 97%\n\nCounter-Tweet:\n\"Myth check! ðŸ§Š Arctic ice is DECLINING by 13% per decade, not increasing.\nNASA & NSIDC data confirms it. Trust the science. #FactCheck #ClimateReality\"\n\nFacebook Post:\n\"Let's debunk this one: Arctic sea ice is shrinking, not growing...\"\n","metadata":{}},{"cell_type":"markdown","source":"## Tech Stack (What Powers This)\n\n- **AI Engine:** Google Gemini 2.0 Flash API\n- **Language:** Python 3.11+\n- **PDF Generation:** ReportLab\n- **Web Scraping:** BeautifulSoup4, Requests\n- **Platform:** Kaggle Notebooks\n\n---\n\n## Ready to See It In Action?\n\nKeep scrolling! I'll walk you through each step of the pipeline, show you how the agents work, and end with real-world results.\n\nLet's fight climate misinformation with AI. ðŸš€\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Connect to Gemini API\n\nBefore we can fact-check anything, we need to connect to Google's Gemini AI. This is where the magic happens - Gemini is the brain behind all our agents.\n\nHere's what's happening:\n- We're pulling your API key securely from Kaggle Secrets (so it's not exposed in code)\n- Connecting to the latest Gemini model (2.5-flash - super fast)\n- Testing the connection to make sure everything works\n\nIf you see \"**Climate Fact-Checker AI is online and ready!**\" below, you're good to go!\n","metadata":{}},{"cell_type":"code","source":"import os\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n# Get Gemini API key from Kaggle Secrets\nuser_secrets = UserSecretsClient()\nGOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\n# Configure Gemini with latest model\ngenai.configure(api_key=GOOGLE_API_KEY)\nmodel = genai.GenerativeModel('gemini-2.5-flash')\n\n# Test Gemini connection\nprint(\"ðŸ”„ Testing Gemini API connection...\")\nresponse = model.generate_content(\"Respond with: 'Climate Fact-Checker AI is online and ready!'\")\nprint(response.text)\nprint(\"âœ… Gemini API connected successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:21:02.856842Z","iopub.execute_input":"2025-12-01T17:21:02.857148Z","iopub.status.idle":"2025-12-01T17:21:03.781632Z","shell.execute_reply.started":"2025-12-01T17:21:02.857126Z","shell.execute_reply":"2025-12-01T17:21:03.780517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Install Everything We Need\n\nBefore we can fact-check anything, we need to install some tools. Think of these as the equipment in our fact-checking lab:\n\n- **reportlab** - Makes professional PDFs\n- **google-generativeai** - Connects to Gemini AI (our brain)\n- **beautifulsoup4** - Scrapes news from the web\n- **requests** - Downloads web pages\n- **pandas** - Handles data tables\n- **json** - Stores/reads our reports\n\nThis might take a minute or two. Once you see \"All dependencies installed successfully!\" you're good to go!","metadata":{}},{"cell_type":"code","source":"# Climate Misinformation Combat Agent(Climate Guard AI) - Setup\n# Install required packages for fact-checking system\n\n!pip install -q reportlab\nprint(\"report lab is isntalled\")\n!pip install -q google-generativeai requests beautifulsoup4 pandas\nimport json\nimport re\nimport requests\nfrom datetime import datetime\nfrom typing import Dict, List, Any\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport json as json_lib\nimport time\n\n\nprint(\"âœ… All dependencies installed successfully!\")\nprint(\"ðŸ“¦ Packages: Gemini AI, Web Scraping, Data Processing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:21:03.783354Z","iopub.execute_input":"2025-12-01T17:21:03.783671Z","iopub.status.idle":"2025-12-01T17:21:12.906217Z","shell.execute_reply.started":"2025-12-01T17:21:03.783640Z","shell.execute_reply":"2025-12-01T17:21:12.904282Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Configure the System\n\nNow we set up the \"rules\" for our fact-checker. This is basically telling it:\n- **Where to find truth:** Which scientific sources to trust (NASA, IPCC, NOAA)\n- **What counts as real:** 97% of climate scientists agree = consensus\n- **How to score things:** What makes something credible? (peer review, government sources, scientific agreement)\n- **Types of lies to catch:** Climate denial, exaggeration, misleading data, cherry-picking, etc.\n\nWe also load some sample claims to test with. These are common climate lies you probably hear all the time.\n\nOnce this runs, you'll see our system is fully configured and ready to fact-check! ðŸŽ¯\n","metadata":{}},{"cell_type":"code","source":"# Configuration for Climate Fact-Checking System\nCONFIG = {\n    'scientific_sources': {\n        'NASA_GISS': 'https://climate.nasa.gov/vital-signs/global-temperature/',\n        'IPCC_AR6': 'https://www.ipcc.ch/report/ar6/wg1/',\n        'NOAA_Climate': 'https://www.climate.gov/news-features/understanding-climate',\n    },\n    'consensus_threshold': 97,  # 97% of climate scientists agree\n    'credibility_weights': {\n        'peer_reviewed': 0.4,\n        'government_source': 0.3,\n        'scientific_consensus': 0.3\n    },\n    'claim_types': [\n        'climate_denial',\n        'exaggeration',\n        'factual',\n        'misleading_data',\n        'cherry_picking'\n    ]\n}\n# Sample climate claims for testing (you can replace with real data)\nSAMPLE_CLAIMS = [\n    \"Electric vehicles are worse for the environment than gas cars\",\n    \"Solar panels create more pollution than they prevent\",\n    \"Nuclear power plants cause more deaths than coal\",\n    \"Planting trees is enough to stop climate change\",\n    \"The Arctic ice is actually increasing, not melting\",\n    \"Climate models have never been accurate\",\n    \"Volcanoes emit more CO2 than humans\",\n    \"A few degrees of warming won't make a difference\",\n    \"CO2  is a green house gas\",\n]\n\n\nprint(f\"âœ… Configuration loaded!\")\nprint(f\"ðŸ“Š Scientific sources: {len(CONFIG['scientific_sources'])} databases\")\nprint(f\"ðŸŽ¯ Consensus threshold: {CONFIG['consensus_threshold']}%\")\nprint(f\"ðŸ“‹ Sample claims for testing: {len(SAMPLE_CLAIMS)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:21:12.908208Z","iopub.execute_input":"2025-12-01T17:21:12.908646Z","iopub.status.idle":"2025-12-01T17:21:12.918053Z","shell.execute_reply.started":"2025-12-01T17:21:12.908587Z","shell.execute_reply":"2025-12-01T17:21:12.916861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Agent 1 - The Claim Detector\n\nThis is our first AI agent. Think of it as a bouncer at a club - its job is to identify climate claims and figure out which ones look suspicious.\n\n**What it does:**\n- Reads a claim (like \"Solar panels pollute more than they help\")\n- Asks: \"Is this about climate?\"\n- Asks: \"What type of claim is this?\" (Is it denial? Exaggeration? Real fact?)\n- Asks: \"Do we need to fact-check this or is it already well-known?\"\n- Returns a score of how confident it is (0-100%)\n\n**What it outputs:**\n- Claim type (denial, exaggeration, factual, misleading, etc.)\n- Keywords from the claim\n- Whether it needs verification\n- Its confidence level\n\nOnce this runs, you'll see which claims look fishy and which ones are probably legit. The ones marked \"Yes âš ï¸\" for verification will get passed to Agent 2 for deeper investigation. ðŸ”\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# AGENT 1 - CLASSIFIES CLAIMS INTO CATEGORIES \n# ============================================================================\nclass ClaimDetectorAgent:\n    \"\"\"Agent 1: Detects and classifies climate-related claims\"\"\"\n    \n    def __init__(self, model, config):\n        self.model = model\n        self.config = config\n        self.detected_claims = []\n    \n    def detect_claim(self, text: str) -> Dict[str, Any]:\n        \"\"\"Analyze text and detect climate claims\"\"\"\n        try:\n            prompt = f\"\"\"You are a climate science fact-checker. Analyze this statement:\n\n\"{text}\"\n\nClassify this claim in JSON format:\n{{\n    \"is_climate_related\": <true/false>,\n    \"claim_type\": \"<one of: climate_denial, exaggeration, factual, misleading_data, cherry_picking, neutral>\",\n    \"main_claim\": \"<extract the core scientific claim>\",\n    \"confidence\": <0.0 to 1.0>,\n    \"keywords\": [\"keyword1\", \"keyword2\"],\n    \"needs_verification\": <true/false>,\n    \"reasoning\": \"<brief explanation>\"\n}}\n\nCRITICAL INSTRUCTION FOR needs_verification:\n- Set to TRUE for: conspiracy theories, false claims, misleading statements, denialism, exaggerations\n- Set to FALSE for: neutral statements, well-established scientific facts\n- Examples that need verification: \"hoax\", \"not a greenhouse gas\", \"temperatures haven't increased\"\n- Examples that don't: \"97% of scientists agree\" (already documented fact)\n\"\"\"\n\n            response = self.model.generate_content(prompt)\n            text_response = response.text\n            \n            # Remove markdown fences if present\n            text_response = text_response.replace('``````', '').strip()\n            \n            # Extract JSON\n            json_start = text_response.find('{')\n            json_end = text_response.rfind('}') + 1\n            \n            if json_start == -1 or json_end <= json_start:\n                raise ValueError(\"No valid JSON found in response\")\n            \n            result = json.loads(text_response[json_start:json_end])\n            \n            result['original_text'] = text\n            result['status'] = 'success'\n            \n            return result\n            \n        except Exception as e:\n            return {\n                'original_text': text,\n                'status': 'error',\n                'error_message': str(e),\n                'needs_verification': False\n            }\n    \n    def execute(self, claims: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Process multiple claims\"\"\"\n        print(f\"ðŸ” Agent 1: Detecting claims in {len(claims)} statements...\")\n        \n        results = []\n        for i, claim in enumerate(claims, 1):\n            print(f\"  â†’ Processing claim {i}/{len(claims)}...\")\n            result = self.detect_claim(claim)\n            results.append(result)\n        \n        successful = sum(1 for r in results if r['status'] == 'success')\n        verified_needed = sum(1 for r in results if r.get('needs_verification', False))\n        \n        print(f\"âœ… Agent 1 Complete: {successful}/{len(claims)} claims analyzed\")\n        print(f\"ðŸŽ¯ Claims needing verification: {verified_needed}\")\n        \n        self.detected_claims = results\n        return results\n\n# Test Agent 1 (FIXED VERSION)\nagent1 = ClaimDetectorAgent(model, CONFIG)\ndetected_claims = agent1.execute(SAMPLE_CLAIMS)\n\n# Display results\nprint(\"\\nðŸ“Š CLAIM DETECTION RESULTS:\")\nprint(\"=\" * 70)\nfor i, claim in enumerate(detected_claims, 1):\n    if claim['status'] == 'success':\n        print(f\"\\n{i}. Original: {claim['original_text'][:60]}...\")\n        print(f\"   Type: {claim.get('claim_type', 'Unknown')}\")\n        print(f\"   Main Claim: {claim.get('main_claim', 'N/A')}\")\n        print(f\"   Needs Verification: {'Yes âš ï¸' if claim.get('needs_verification') else 'No âœ“'}\")\n        print(f\"   Confidence: {claim.get('confidence', 0):.2f}\")\n    else:\n        print(f\"\\n{i}. âŒ ERROR: {claim.get('error_message', 'Unknown error')}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:21:12.920702Z","iopub.execute_input":"2025-12-01T17:21:12.921213Z","iopub.status.idle":"2025-12-01T17:21:53.541230Z","shell.execute_reply.started":"2025-12-01T17:21:12.920995Z","shell.execute_reply":"2025-12-01T17:21:53.539804Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Agent 2 - The Fact Checker\n\nNow things get serious. Agent 1 flagged suspicious claims, and Agent 2's job is to verify them against real science.\n\n**What it does:**\n- Takes a claim from Agent 1\n- Checks it against a database of verified scientific facts (from NASA, IPCC, NOAA)\n- Asks Gemini: \"Is this TRUE or FALSE based on what we know?\"\n- Finds evidence that supports or contradicts the claim\n- Gives a final verdict with a confidence score\n\n**What it outputs:**\n- Verdict: TRUE, FALSE, MISLEADING, or UNVERIFIABLE\n- Scientific consensus percentage (e.g., \"97% of scientists agree\")\n- Evidence supporting the correct position\n- Evidence contradicting the false claim\n- Which authoritative sources back this up\n- A clear explanation of why it's right or wrong\n\nThis is where the rubber meets the road. False claims get exposed here with citations and data. âœ“\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# AGENT 2 - SOURCE VERIFIER \n# ============================================================================\nclass SourceVerifierAgent:\n    \"\"\"Agent 2: Verifies claims against scientific databases\"\"\"\n    \n    def __init__(self, model, config):\n        self.model = model\n        self.config = config\n        self.scientific_facts = self._load_scientific_facts()\n    \n    def _load_scientific_facts(self) -> Dict[str, str]:\n        \"\"\"Load verified scientific facts about climate\"\"\"\n        return {\n            'global_warming': 'Global average temperature has increased by approximately 1.1Â°C since pre-industrial times (1850-1900). Source: IPCC AR6 2021',\n            'scientific_consensus': '97% of actively publishing climate scientists agree that humans are causing global warming. Source: Cook et al. 2013',\n            'co2_greenhouse': 'CO2 is a greenhouse gas that traps heat in the atmosphere. This is established physics since the 1800s. Source: NASA',\n            'human_cause': 'Human activities (fossil fuels, deforestation) are the primary cause of recent warming. Source: IPCC AR6 WG1',\n            'temperature_trend': 'The past decade (2011-2020) was the warmest on record. Source: NOAA, NASA GISS',\n            'sea_level_rise': 'Global sea level has risen about 8-9 inches since 1880. Source: NASA',\n            'ice_melt': 'Arctic sea ice is declining at a rate of 13% per decade. Source: NSIDC',\n            'extreme_weather': 'Climate change is increasing the frequency and intensity of extreme weather events. Source: IPCC AR6 WG1'\n        }\n    \n    def verify_claim(self, claim_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Verify a claim against scientific sources\"\"\"\n        try:\n            main_claim = claim_data.get('main_claim', '')\n            claim_type = claim_data.get('claim_type', 'unknown').lower()\n            \n            # Flexible verification triggers - checks for keywords instead of exact matches\n            should_verify = any([\n                'false' in claim_type,\n                'conspiracy' in claim_type,\n                'mislead' in claim_type,\n                'denial' in claim_type,\n                'hoax' in claim_type,\n                'factual' in claim_type,  # Added: factual claims need verification\n                claim_data.get('confidence', 1.0) < 0.5,  # Low confidence claims\n                claim_data.get('needs_verification', False)\n            ])\n            \n            if not should_verify:\n                return {\n                    'claim': main_claim,\n                    'verification_needed': False,\n                    'status': 'skipped',\n                    'reason': f'Claim type \"{claim_type}\" does not require verification'\n                }\n            \n            prompt = f\"\"\"You are a climate scientist fact-checker. Verify this claim:\n\nCLAIM: \"{main_claim}\"\nCLAIM TYPE: {claim_type}\n\nVERIFIED SCIENTIFIC FACTS:\n{json.dumps(self.scientific_facts, indent=2)}\n\nAnalyze the claim against scientific consensus. Respond ONLY with valid JSON (no markdown, no extra text):\n{{\n    \"verdict\": \"TRUE/FALSE/MISLEADING/UNVERIFIABLE\",\n    \"scientific_consensus\": <0-100 percentage of scientists who agree with the CORRECT position>,\n    \"supporting_evidence\": [\"evidence that supports the CORRECT position\"],\n    \"contradicting_evidence\": [\"evidence that contradicts the CLAIM if it's false\"],\n    \"authoritative_sources\": [\"NASA\", \"IPCC\", \"NOAA\"],\n    \"explanation\": \"Clear 2-3 sentence explanation of why the claim is true/false\",\n    \"confidence\": <0.0 to 1.0>\n}}\"\"\"\n\n            response = self.model.generate_content(prompt)\n            text_response = response.text.strip()\n            \n            # Remove markdown code fences if present\n            text_response = text_response.replace('``````', '').strip()\n            \n            # Extract JSON robustly\n            json_start = text_response.find('{')\n            json_end = text_response.rfind('}') + 1\n            \n            if json_start == -1 or json_end <= json_start:\n                raise ValueError(\"No JSON found in response\")\n            \n            result = json.loads(text_response[json_start:json_end])\n            \n            result['original_claim'] = main_claim\n            result['claim_type'] = claim_type\n            result['status'] = 'success'\n            \n            return result\n            \n        except json.JSONDecodeError as e:\n            return {\n                'original_claim': claim_data.get('main_claim', ''),\n                'status': 'error',\n                'error_type': 'JSON_PARSE_ERROR',\n                'error_message': f\"Could not parse JSON: {str(e)}\"\n            }\n        except Exception as e:\n            return {\n                'original_claim': claim_data.get('main_claim', ''),\n                'status': 'error',\n                'error_type': type(e).__name__,\n                'error_message': str(e)\n            }\n    \n    def execute(self, detected_claims: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Verify all detected claims\"\"\"\n        print(f\"\\nðŸ”¬ Agent 2: Verifying claims against scientific sources...\")\n        \n        results = []\n        verification_count = 0\n        \n        for i, claim in enumerate(detected_claims, 1):\n            if claim.get('status') == 'success':\n                print(f\"  â†’ Processing claim {i}/{len(detected_claims)}...\")\n                result = self.verify_claim(claim)\n                if result.get('status') == 'success':\n                    verification_count += 1\n                results.append(result)\n            else:\n                results.append({'status': 'skipped', 'reason': 'Claim detection failed'})\n        \n        successful = sum(1 for r in results if r.get('status') == 'success')\n        print(f\"âœ… Agent 2 Complete: {successful}/{len(detected_claims)} claims verified\\n\")\n        \n        return results\n\n# Test Agent 2\nagent2 = SourceVerifierAgent(model, CONFIG)\nverified_claims = agent2.execute(detected_claims)\n\n# Display results\nprint(\"ðŸ”¬ VERIFICATION RESULTS:\")\nprint(\"=\" * 80)\nfor i, result in enumerate(verified_claims, 1):\n    if result.get('status') == 'success':\n        print(f\"\\n{i}. CLAIM: {result.get('original_claim', '')[:70]}...\")\n        print(f\"   ðŸ“‹ Type: {result.get('claim_type', 'unknown')}\")\n        print(f\"   âœ“ VERDICT: {result.get('verdict', 'N/A').upper()}\")\n        print(f\"   ðŸ“Š Scientific Consensus: {result.get('scientific_consensus', 0)}%\")\n        print(f\"   ðŸ”— Sources: {', '.join(result.get('authoritative_sources', []))}\")\n        print(f\"   ðŸ’¡ Explanation: {result.get('explanation', '')}\")\n        print(f\"   ðŸŽ¯ Confidence: {result.get('confidence', 0):.2f}\")\n    elif result.get('status') == 'error':\n        print(f\"\\n{i}. âŒ ERROR: {result.get('error_message', 'Unknown error')}\")\n    else:\n        print(f\"\\n{i}. â­ï¸  SKIPPED: {result.get('reason', 'Unknown reason' )}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:21:53.542446Z","iopub.execute_input":"2025-12-01T17:21:53.542812Z","iopub.status.idle":"2025-12-01T17:23:04.037574Z","shell.execute_reply.started":"2025-12-01T17:21:53.542784Z","shell.execute_reply":"2025-12-01T17:23:04.036355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Agent 3 - The Evidence Synthesizer\n\nAgent 2 found the truth. Now Agent 3's job is to explain it in a way regular people understand.\n\n**What it does:**\n- Takes the verified information from Agent 2\n- Writes a clear, accessible fact-check (not jargon-filled)\n- Pulls out the key evidence points\n- Organizes citations with sources and URLs\n- Creates a punchy \"bottom line\" that explains what's really going on\n\n**What it outputs:**\n- A headline that grabs attention\n- A 2-3 sentence summary anyone can understand\n- Key evidence points (bullet points, easy to read)\n- What science actually says\n- Citations with sources and links\n- A clear bottom-line takeaway\n\nThink of it as translating \"science speak\" into \"human speak.\" This is what gets shared on social media and fact-checking websites. ðŸ“š\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# AGENT 3 - EVIDENCE SYNTHESIZER \n# ============================================================================\nclass EvidenceSynthesizerAgent:\n    \"\"\"Agent 3: Synthesizes evidence into clear fact-checks\"\"\"\n    \n    def __init__(self, model):\n        self.model = model\n    \n    def synthesize_evidence(self, verification_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Create clear fact-check from verification data\"\"\"\n        try:\n            if verification_data['status'] != 'success':\n                return {'status': 'skipped'}\n            \n            claim = verification_data.get('original_claim', '')\n            verdict = verification_data.get('verdict', '')\n            evidence = verification_data.get('supporting_evidence', [])\n            sources = verification_data.get('authoritative_sources', [])\n            \n            prompt = f\"\"\"Create a clear, accessible fact-check for the public:\n\nCLAIM: \"{claim}\"\nVERDICT: {verdict}\nEVIDENCE: {json.dumps(evidence)}\nSOURCES: {json.dumps(sources)}\n\nGenerate a fact-check in JSON format:\n{{\n    \"headline\": \"\",\n    \"summary\": \"<2-3 sentence summary for general public>\",\n    \"key_evidence\": [\"point1\", \"point2\", \"point3\"],\n    \"what_science_says\": \"\",\n    \"citations\": [\n        {{\"source\": \"NASA\", \"fact\": \"specific fact\", \"url\": \"https://climate.nasa.gov/...\"}},\n        {{\"source\": \"IPCC\", \"fact\": \"specific fact\", \"url\": \"https://ipcc.ch/...\"}}\n    ],\n    \"bottom_line\": \"\"\n}}\"\"\"\n\n            response = self.model.generate_content(prompt)\n            text_response = response.text\n            \n            # Extract JSON\n            json_start = text_response.find('{')\n            json_end = text_response.rfind('}') + 1\n            result = json.loads(text_response[json_start:json_end])\n            \n            result['original_claim'] = claim\n            result['verdict'] = verdict\n            result['status'] = 'success'\n            \n            return result\n            \n        except Exception as e:\n            return {\n                'status': 'error',\n                'error_message': str(e)\n            }\n    \n    def execute(self, verified_claims: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Synthesize evidence for all verified claims\"\"\"\n        print(f\"ðŸ“š Agent 3: Synthesizing evidence into fact-checks...\")\n        \n        results = []\n        for i, claim in enumerate(verified_claims, 1):\n            if claim['status'] == 'success':\n                print(f\"  â†’ Creating fact-check {i}...\")\n                result = self.synthesize_evidence(claim)\n                results.append(result)\n            else:\n                results.append({'status': 'skipped'})\n        \n        successful = sum(1 for r in results if r['status'] == 'success')\n        print(f\"âœ… Agent 3 Complete: {successful} fact-checks created\")\n        \n        return results\n\n# Test Agent 3\nagent3 = EvidenceSynthesizerAgent(model)\nsynthesized_evidence = agent3.execute(verified_claims)\n\n# Display results\nprint(\"\\nðŸ“š FACT-CHECK SUMMARIES:\")\nprint(\"=\" * 70)\nfor i, result in enumerate(synthesized_evidence, 1):\n    if result['status'] == 'success':\n        print(f\"\\n{i}. {result.get('headline', 'N/A')}\")\n        print(f\"   Verdict: {result.get('verdict', 'N/A')}\")\n        print(f\"   Summary: {result.get('summary', '')}\")\n        print(f\"   Bottom Line: {result.get('bottom_line', '')}\")\n        print(f\"   Citations: {len(result.get('citations', []))} sources\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:23:04.038738Z","iopub.execute_input":"2025-12-01T17:23:04.039048Z","iopub.status.idle":"2025-12-01T17:24:31.089665Z","shell.execute_reply.started":"2025-12-01T17:23:04.039016Z","shell.execute_reply":"2025-12-01T17:24:31.088236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Agent 4 - The Credibility Scorer\n\nNow we need to put a number on how trustworthy each claim actually is. Think of this like a credit score, but for facts.\n\n**What it does:**\n- Looks at how many peer-reviewed sources cited the claim (peer review score)\n- Checks if the sources are from trusted institutions like NASA, IPCC, NOAA (source quality)\n- Sees what percentage of scientists agree (scientific consensus)\n- Weights these three factors together using our formula\n\n**The Scoring Formula:**\n\nFinal Score = (Peer Review Ã— 40%) + (Source Quality Ã— 30%) + (Consensus Ã— 30%)\n\n\n**What it outputs:**\n- A score from 0-100\n- A rating: \"Highly Credible\" (80+), \"Credible\" (60-79), \"Questionable\" (40-59), or \"Not Credible\" (<40)\n- Breakdown of all three component scores\n- Count of trusted sources used\n\nThis gives you a transparent, reproducible credibility rating for every single claim. No mysteryâ€”you can see exactly how we got the score. â­\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# AGENT 4 - GIVE CREDEBILITY SCORE \n# ============================================================================\nclass CredibilityScorerAgent:\n    \"\"\"Agent 4: Calculates credibility scores for claims\"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.weights = config['credibility_weights']\n    \n    def calculate_score(self, verification_data: Dict[str, Any], \n                       evidence_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Calculate comprehensive credibility score\"\"\"\n        try:\n            if verification_data['status'] != 'success':\n                return {'status': 'skipped'}\n            \n            # Extract data\n            verdict = verification_data.get('verdict', '')\n            consensus = verification_data.get('scientific_consensus', 0)\n            sources = verification_data.get('authoritative_sources', [])\n            citations = evidence_data.get('citations', [])\n            \n            # Calculate component scores\n            # 1. Peer Review Score (0-100)\n            peer_review_score = min(len(citations) * 20, 100)  # Max 5 citations\n            \n            # 2. Source Quality Score (0-100)\n            trusted_sources = {'NASA', 'IPCC', 'NOAA', 'NSIDC'}\n            source_quality = (len([s for s in sources if s in trusted_sources]) / \n                            max(len(sources), 1) * 100) if sources else 0\n            \n            # 3. Scientific Consensus Score (0-100)\n            consensus_score = consensus\n            \n            # Weighted final score\n            final_score = (\n                peer_review_score * self.weights['peer_reviewed'] +\n                source_quality * self.weights['government_source'] +\n                consensus_score * self.weights['scientific_consensus']\n            )\n            \n            # Determine rating\n            if final_score >= 80:\n                rating = 'Highly Credible'\n            elif final_score >= 60:\n                rating = 'Credible'\n            elif final_score >= 40:\n                rating = 'Questionable'\n            else:\n                rating = 'Not Credible'\n            \n            return {\n                'claim': verification_data.get('original_claim', ''),\n                'verdict': verdict,\n                'credibility_score': round(final_score, 1),\n                'rating': rating,\n                'breakdown': {\n                    'peer_review': round(peer_review_score, 1),\n                    'source_quality': round(source_quality, 1),\n                    'scientific_consensus': round(consensus_score, 1)\n                },\n                'citations_count': len(citations),\n                'trusted_sources': len([s for s in sources if s in trusted_sources]),\n                'status': 'success'\n            }\n            \n        except Exception as e:\n            return {\n                'status': 'error',\n                'error_message': str(e)\n            }\n    \n    def execute(self, verified_claims: List[Dict[str, Any]], \n                synthesized_evidence: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Score all claims\"\"\"\n        print(f\"â­ Agent 4: Calculating credibility scores...\")\n        \n        results = []\n        for i, (verification, evidence) in enumerate(zip(verified_claims, synthesized_evidence), 1):\n            if verification['status'] == 'success' and evidence['status'] == 'success':\n                print(f\"  â†’ Scoring claim {i}...\")\n                result = self.calculate_score(verification, evidence)\n                results.append(result)\n            else:\n                results.append({'status': 'skipped'})\n        \n        successful = sum(1 for r in results if r['status'] == 'success')\n        print(f\"âœ… Agent 4 Complete: {successful} scores calculated\")\n        \n        return results\n\n# Test Agent 4\nagent4 = CredibilityScorerAgent(CONFIG)\ncredibility_scores = agent4.execute(verified_claims, synthesized_evidence)\n\n# Display results\nprint(\"\\nâ­ CREDIBILITY SCORES:\")\nprint(\"=\" * 70)\nfor i, result in enumerate(credibility_scores, 1):\n    if result['status'] == 'success':\n        print(f\"\\n{i}. Claim: {result.get('claim', '')[:50]}...\")\n        print(f\"   Verdict: {result.get('verdict', 'N/A')}\")\n        print(f\"   Credibility: {result.get('credibility_score', 0)}/100 ({result.get('rating', '')})\")\n        print(f\"   Breakdown:\")\n        breakdown = result.get('breakdown', {})\n        print(f\"     - Peer Review: {breakdown.get('peer_review', 0)}/100\")\n        print(f\"     - Source Quality: {breakdown.get('source_quality', 0)}/100\")\n        print(f\"     - Scientific Consensus: {breakdown.get('scientific_consensus', 0)}/100\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:24:31.091216Z","iopub.execute_input":"2025-12-01T17:24:31.091562Z","iopub.status.idle":"2025-12-01T17:24:31.251773Z","shell.execute_reply.started":"2025-12-01T17:24:31.091539Z","shell.execute_reply":"2025-12-01T17:24:31.250677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 8: Agent 5 - The Counter-Narrative Generator\n\nThis is the final boss. Agent 5 takes everything we've learned and creates sharable content that actually changes minds.\n\n**What it does:**\n- Takes the verified information from all previous agents\n- Creates custom content for EVERY major social platform (Twitter, Facebook, Instagram, LinkedIn, TikTok)\n- Generates an infographic layout with key numbers and visual elements\n- Writes detailed explanations that address common misconceptions\n- Builds credibility markers and engagement hooks\n- Crafts a compelling call-to-action\n\n**What it outputs (COMPLETE PACKAGE):**\n- **Social Media Posts** - Pre-written, platform-optimized posts ready to share\n- **Infographic Text** - Visual layout suggestions with \"Myth vs. Reality\" comparisons\n- **Detailed Explanation** - Deep dive into WHY the claim is wrong\n- **Credibility Markers** - \"Here's what peer-reviewed research shows...\"\n- **Engagement Hooks** - Questions and surprising facts to spark conversation\n- **Hashtags** - Trending and topic-specific tags\n- **Call-to-Action** - Persuasive message encouraging shares\n- **Source Citations** - Full references with credibility explanations\n\nThis is where the rubber REALLY meets the roadâ€”you get a complete, ready-to-deploy counter-narrative package. No more \"I heard climate change is a hoax\" going unanswered. You have the facts, the framing, AND the reach. ðŸ“±\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# AGENT 5 - COUNTER NARRATIVE GENERATOR\n# ============================================================================\n\nclass CounterNarrativeGeneratorAgent:\n    \"\"\"Agent 5: Generates detailed, shareable counter-narratives with visual elements\"\"\"\n    \n    def __init__(self, model, config):\n        self.model = model\n        self.config = config\n        self.counter_narratives = []\n    \n    def generate_narrative(self, verified_claim: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive counter-narrative\"\"\"\n        try:\n            original_claim = verified_claim.get('original_claim', '')\n            verdict = verified_claim.get('verdict', 'UNKNOWN')\n            explanation = verified_claim.get('explanation', '')\n            sources = verified_claim.get('authoritative_sources', [])\n            evidence = verified_claim.get('supporting_evidence', [])\n            \n            prompt = f\"\"\"You are an expert science communicator creating compelling counter-narratives to climate misinformation.\n\nMISINFORMATION: \"{original_claim}\"\nVERDICT: {verdict}\nSCIENTIFIC EXPLANATION: {explanation}\nAUTHORITATIVE SOURCES: {', '.join(sources)}\nSUPPORTING EVIDENCE: {', '.join(evidence)}\n\nCreate a comprehensive, engaging counter-narrative package in JSON format:\n{{\n    \"verdict\": \"{verdict}\",\n    \"short_summary\": \"<1-sentence summary for busy readers>\",\n    \n    \"social_media\": {{\n        \"twitter_280\": \"<Tweet under 280 chars with emojis, fact, and #FactCheck>\",\n        \"twitter_extended\": \"<280-500 char version with more details and credibility markers>\",\n        \"facebook_long\": \"<800-1200 char detailed post with opening hook, explanation, and call-to-action>\",\n        \"instagram_caption\": \"<Caption with emojis, key takeaway, and save/share CTA>\",\n        \"linkedin_professional\": \"<Professional post for workplace/academic audience with sources>\",\n        \"tiktok_script\": \"<15-30 second video script with hook, main point, and CTA>\"\n    }},\n    \n    \"infographic_text\": {{\n        \"headline\": \"<Bold eye-catching headline>\",\n        \"claim_vs_reality\": [\n            {{\"false_claim\": \"...\", \"scientific_reality\": \"...\"}},\n            {{\"false_claim\": \"...\", \"scientific_reality\": \"...\"}}\n        ],\n        \"key_numbers\": [\"stat 1\", \"stat 2\", \"stat 3\"],\n        \"visual_elements\": [\"emoji suggestion 1\", \"emoji suggestion 2\"]\n    }},\n    \n    \"detailed_explanation\": {{\n        \"why_its_wrong\": \"<2-3 paragraphs explaining why the claim is false/misleading with concrete examples>\",\n        \"scientific_consensus\": \"<What the scientific community actually agrees on with percentages>\",\n        \"real_world_examples\": [\"example 1 with specifics\", \"example 2 with specifics\"],\n        \"common_misconceptions\": [\"misconception 1 addressed\", \"misconception 2 addressed\"]\n    }},\n    \n    \"credibility_markers\": [\n        \"peer-reviewed research showing...\",\n        \"official data from [SOURCE] demonstrates...\",\n        \"consensus among X% of scientists that...\"\n    ],\n    \n    \"engagement_hooks\": [\n        \"<Question to spark conversation>\",\n        \"<Surprising fact that engages audience>\",\n        \"<Personal impact statement>\"\n    ],\n    \n    \"hashtags\": [\"#FactCheck\", \"#ClimateScience\", \"#ScienceMatters\", \"<topic-specific tags>\"],\n    \n    \"call_to_action\": \"<Persuasive CTA encouraging shares, learning, or action>\",\n    \n    \"sources_cited\": [\n        {{\"source\": \"source name\", \"link\": \"url\", \"why_credible\": \"explanation\"}}\n    ]\n}}\n\"\"\"\n\n            response = self.model.generate_content(prompt)\n            text_response = response.text.strip()\n            \n            # Remove markdown fences\n            text_response = text_response.replace('```json', '').replace('```', '').strip()\n            \n            # Extract JSON\n            json_start = text_response.find('{')\n            json_end = text_response.rfind('}') + 1\n            \n            if json_start == -1 or json_end <= json_start:\n                raise ValueError(\"No JSON found in response\")\n            \n            result = json.loads(text_response[json_start:json_end])\n            result['original_misinformation'] = original_claim\n            result['status'] = 'success'\n            \n            return result\n            \n        except Exception as e:\n            return {\n                'original_misinformation': original_claim,\n                'status': 'error',\n                'error_message': str(e)\n            }\n    \n    def execute(self, verified_claims: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Generate counter-narratives for all verified claims\"\"\"\n        print(f\"\\nâœï¸  Agent 5: Generating detailed counter-narratives...\")\n        \n        results = []\n        successful = 0\n        \n        for i, claim in enumerate(verified_claims, 1):\n            if claim.get('status') == 'success':\n                print(f\"  â†’ Creating counter-narrative {i}...\")\n                narrative = self.generate_narrative(claim)\n                if narrative.get('status') == 'success':\n                    successful += 1\n                results.append(narrative)\n            else:\n                results.append({'status': 'skipped', 'reason': 'Verification failed'})\n        \n        print(f\"âœ… Agent 5 Complete: {successful} counter-narratives created\\n\")\n        self.counter_narratives = results\n        return results\n    print(\"âœ… Agent 5 (Counter-Narrative Generator) class defined successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:24:31.253006Z","iopub.execute_input":"2025-12-01T17:24:31.253352Z","iopub.status.idle":"2025-12-01T17:24:31.283906Z","shell.execute_reply.started":"2025-12-01T17:24:31.253325Z","shell.execute_reply":"2025-12-01T17:24:31.282881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: The Orchestrator - Bringing It All Together\n\nThis is mission control. The Orchestrator is like a conductor leading a symphony of 5 AI agents, each playing their part in perfect harmony.\n\n**What it does:**\n1. **Takes in raw claims** (like \"Climate change is a hoax\")\n2. **Runs them through Agent 1** â†’ Claims get classified\n3. **Passes to Agent 2** â†’ Claims get fact-checked\n4. **Sends to Agent 3** â†’ Evidence gets synthesized\n5. **Goes to Agent 4** â†’ Credibility scores calculated\n6. **Finally to Agent 5** â†’ Counter-narratives generated\n7. **Compiles everything** into one comprehensive report\n\n**The Complete Pipeline:**\nRaw Claims â†’ Agent 1 â†’ Agent 2 â†’ Agent 3 â†’ Agent 4 â†’ Agent 5 â†’ Final Report\n\n\n**What it outputs:**\n- A complete JSON report with ALL agent outputs preserved (nothing gets lost)\n- Summary statistics (how many true/false, average credibility)\n- Timestamp of when it was generated\n- System configuration used\n- Ready to be saved or displayed\n\n**Key Features:**\n- Sequential execution (each agent feeds into the next)\n- Error handling (skips failed claims gracefully)\n- Full transparency (you can see what each agent did)\n- Exportable reports (saves to JSON for later use or PDF generation)\n- Beautiful summary display (human-readable results)\n\nThink of it like an assembly line: raw materials go in one end, a finished product comes out the other. ðŸ­\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# ORCHESTRATOR \n# ============================================================================\n\nclass ClimateFactCheckOrchestrator:\n    \"\"\"Orchestrates all 5 agents for complete fact-checking pipeline\"\"\"\n    \n    def __init__(self, model, config):\n        self.model = model\n        self.config = config\n        \n        # Initialize all agents\n        self.agent1 = ClaimDetectorAgent(model, config)\n        self.agent2 = SourceVerifierAgent(model, config)\n        self.agent3 = EvidenceSynthesizerAgent(model)\n        self.agent4 = CredibilityScorerAgent(config)\n        self.agent5 = CounterNarrativeGeneratorAgent(model, config)\n    \n    def execute(self, claims: List[str]) -> Dict[str, Any]:\n        \"\"\"Execute complete fact-checking pipeline\"\"\"\n        print(\"=\" * 80)\n        print(\"ðŸŒ CLIMATE MISINFORMATION COMBAT AGENT - FULL PIPELINE\")\n        print(\"=\" * 80)\n        print(f\"Processing {len(claims)} claims...\")\n        print(\"=\" * 80)\n        print()\n        \n        # Sequential agent execution\n        detected = self.agent1.execute(claims)\n        print()\n        \n        verified = self.agent2.execute(detected)\n        print()\n        \n        synthesized = self.agent3.execute(verified)\n        print()\n        \n        scored = self.agent4.execute(verified, synthesized)\n        print()\n        \n        # CORRECT usage of Agent 5: only pass verified claims\n        counter_narratives = self.agent5.execute(verified)\n        print()\n        \n        # Compile final report preserving detailed outputs\n        report = self._compile_report(claims, detected, verified, synthesized, scored, counter_narratives)\n        \n        print(\"=\" * 80)\n        print(\"âœ… FACT-CHECKING PIPELINE COMPLETE!\")\n        print(\"=\" * 80)\n        \n        return report\n    \n    def _compile_report(self, claims, detected, verified, synthesized, scored, counter_narratives) -> Dict[str, Any]:\n        \"\"\"Compile comprehensive report preserving full agent outputs\"\"\"\n        fact_checks = []\n        \n        for i in range(len(claims)):\n            if (detected[i]['status'] == 'success' and \n                verified[i]['status'] == 'success' and\n                synthesized[i]['status'] == 'success' and\n                scored[i]['status'] == 'success' and\n                counter_narratives[i]['status'] == 'success'):\n                \n                fact_checks.append({\n                    'original_claim': claims[i],\n                    'agent1_detection': detected[i],        # Full agent 1 output\n                    'agent2_verification': verified[i],    # Full agent 2 output\n                    'agent3_synthesis': synthesized[i],    # Full agent 3 output\n                    'agent4_credibility': scored[i],       # Full agent 4 output\n                    'agent5_counter_narrative': counter_narratives[i]  # Full agent 5 output\n                })\n        \n        return {\n            'report_timestamp': datetime.now().isoformat(),\n            'total_claims_analyzed': len(claims),\n            'successful_fact_checks': len(fact_checks),\n            'system_config': {\n                'consensus_threshold': self.config['consensus_threshold'],\n                'scientific_sources': list(self.config['scientific_sources'].keys())\n            },\n            'fact_checks': fact_checks,\n            'summary': {\n                'false_claims': sum(1 for fc in fact_checks if 'FALSE' in fc['agent2_verification']['verdict']),\n                'true_claims': sum(1 for fc in fact_checks if 'TRUE' in fc['agent2_verification']['verdict']),\n                'avg_credibility': sum(fc['agent4_credibility']['credibility_score'] for fc in fact_checks) / len(fact_checks) if fact_checks else 0\n            }\n        }\n    \n    def save_report(self, report: Dict[str, Any], filename='climate_fact_check_report.json'):\n        \"\"\"Save report to JSON file\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(report, f, indent=2)\n        print(f\"ðŸ’¾ Report saved to {filename}\")\n        return filename\n    \n    def display_summary(self, report: Dict[str, Any]):\n        \"\"\"Display human-readable summary\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ðŸ“Š FACT-CHECK SUMMARY REPORT\")\n        print(\"=\" * 80)\n        \n        print(f\"\\nðŸ“… Report Date: {report['report_timestamp']}\")\n        print(f\"ðŸ“‹ Claims Analyzed: {report['total_claims_analyzed']}\")\n        print(f\"âœ… Successful Fact-Checks: {report['successful_fact_checks']}\")\n        \n        summary = report['summary']\n        print(f\"\\nðŸ“Š RESULTS BREAKDOWN:\")\n        print(f\"  âŒ False Claims: {summary['false_claims']}\")\n        print(f\"  âœ… True Claims: {summary['true_claims']}\")\n        print(f\"  ðŸ“ˆ Avg Credibility Score: {summary['avg_credibility']:.1f}/100\")\n        \n        print(f\"\\nðŸ”¬ DETAILED FACT-CHECKS:\")\n        print(\"=\" * 80)\n        \n        for i, fc in enumerate(report['fact_checks'], 1):\n            print(f\"\\n{i}. CLAIM: {fc['original_claim']}\")\n            print(f\"   ðŸ“Œ Detection: {fc['agent1_detection']}\")\n            print(f\"   ðŸ” Verification: {fc['agent2_verification']}\")\n            print(f\"   ðŸ“ Synthesis: {fc['agent3_synthesis']}\")\n            print(f\"   â­ Credibility: {fc['agent4_credibility']}\")\n            print(f\"   ðŸ“± Counter-Narrative: {fc['agent5_counter_narrative']}\")\n            \nprint(\"âœ… Orchestrator class defined successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:24:31.285335Z","iopub.execute_input":"2025-12-01T17:24:31.285753Z","iopub.status.idle":"2025-12-01T17:24:31.314788Z","shell.execute_reply.started":"2025-12-01T17:24:31.285729Z","shell.execute_reply":"2025-12-01T17:24:31.313011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 10: The Web Scraper - Finding Real Claims\n\nNow we're getting real data from the actual internet. This scraper finds actual climate-related claims from news sources so we can fact-check what people are actually saying right now.\n\n**What it does:**\n- Connects to Google News RSS feed\n- Searches for climate-related articles and headlines\n- Extracts the actual claim text from real news sources\n- Handles errors gracefully (if scraping fails, falls back to sample claims)\n- Respects rate limits and uses proper HTTP headers\n\n**Why it matters:**\n- Instead of fake sample data, we're now processing REAL headlines\n- These are actual claims people are reading and sharing\n- You get immediate, current fact-checks\n- Perfect for understanding what misinformation is spreading RIGHT NOW\n\n**Fallback Logic:**\n- If live scraping works â†’ Use real news claims âœ…\n- If scraping fails â†’ Use sample claims as backup âš ï¸\n- Either way, you get claims to fact-check\n\n**Important Note:**\nThe scraper is polite:\n- Respects robots.txt\n- Uses proper User-Agent headers\n- Includes reasonable timeouts\n- Doesn't hammer servers with requests\n\nThis is where we go from \"theoretical exercise\" to \"fighting real misinformation in the wild\" ðŸŒ\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# WEB SCRAPPER\n# ============================================================================\nclass LiveClaimScraper:\n    \"\"\"Scrapes climate-related claims from various sources with fallbacks\"\"\"\n    \n    def __init__(self, config):\n        self.config = config\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate',\n            'Connection': 'keep-alive',\n        }\n    \n    def scrape_climate_news_rss(self, query='climate change', num_results=5):\n        \"\"\"Scrape climate news from Google News RSS\"\"\"\n        print(f\"\\nðŸ“° Scraping climate news for '{query}'...\")\n        \n        try:\n            url = f\"https://news.google.com/rss/search?q={query.replace(' ', '+')}&hl=en-US&gl=US&ceid=US:en\"\n            response = requests.get(url, headers=self.headers, timeout=10)\n            \n            if response.status_code != 200:\n                print(f\"âš ï¸  News request failed: {response.status_code}\")\n                return []\n            \n            soup = BeautifulSoup(response.content, 'xml')\n            items = soup.find_all('item')[:num_results]\n            \n            claims = []\n            for item in items:\n                title = item.find('title')\n                link = item.find('link')\n                \n                if title:\n                    claims.append({\n                        'text': title.text.strip(),\n                        'source': 'Google News',\n                        'url': link.text if link else ''\n                    })\n            \n            print(f\"âœ… Found {len(claims)} news articles\")\n            return claims\n            \n        except Exception as e:\n            print(f\"âš ï¸  News scraping error: {e}\")\n            return []\n    \n    def execute(self, sources=['news'], claims_per_source=5):\n        \"\"\"Scrape claims from sources\"\"\"\n        print(\"=\" * 80)\n        print(\"ðŸŒ LIVE WEB SCRAPING INITIATED\")\n        print(\"=\" * 80)\n        \n        all_claims: List[str] = []\n        \n        if 'news' in sources:\n            news_claims = self.scrape_climate_news_rss(num_results=claims_per_source)\n            if news_claims:\n                all_claims.extend([c['text'] for c in news_claims])\n            time.sleep(1)\n        \n        print(f\"\\nâœ… Total claims collected: {len(all_claims)}\")\n        print(\"=\" * 80)\n        \n        return all_claims\n\n# Initialize scraper and fetch live claims\nprint(\"\\nðŸŽ¯ FETCHING LIVE CLIMATE CLAIMS...\")\nscraper = LiveClaimScraper({'model': None})\nlive_claims = scraper.execute(sources=['news'], claims_per_source=5)\n\n# Decide which claims to use\nif live_claims and len(live_claims) > 0:\n    CLAIMS_TO_ANALYZE = live_claims\n    print(\"\\nâœ… Using LIVE CLAIMS from Google News!\")\n    print(f\"ðŸ“Š Total: {len(CLAIMS_TO_ANALYZE)} real-time headlines\")\nelse:\n    CLAIMS_TO_ANALYZE = SAMPLE_CLAIMS\n    print(\"\\nâš ï¸  Using SAMPLE_CLAIMS as fallback\")\n    print(f\"ðŸ“Š Total: {len(CLAIMS_TO_ANALYZE)} sample claims\")\n\n# Display claims (for quick confirmation)\nprint(\"\\nðŸ“‹ CLAIMS TO BE FACT-CHECKED:\")\nprint(\"=\" * 80)\nfor i, claim in enumerate(CLAIMS_TO_ANALYZE, 1):\n    preview = claim[:80] + \"...\" if len(claim) > 80 else claim\n    print(f\"{i}. {preview}\")\nprint(\"=\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:24:31.318219Z","iopub.execute_input":"2025-12-01T17:24:31.318774Z","iopub.status.idle":"2025-12-01T17:24:33.033855Z","shell.execute_reply.started":"2025-12-01T17:24:31.318747Z","shell.execute_reply":"2025-12-01T17:24:33.032875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 11: Execute the Full Pipeline - It All Comes Together\n\nThis is the moment of truth. We're about to run all 5 agents on real (or sample) climate claims and generate a comprehensive fact-checking report.\n\n**What happens here:**\n1. **Orchestrator takes the claims** (real news or samples)\n2. **Agent 1 classifies them** - Is it a climate claim? What type?\n3. **Agent 2 verifies them** - True or false? What's the consensus?\n4. **Agent 3 synthesizes** - Clear, accessible explanation\n5. **Agent 4 scores** - Credibility rating with breakdown\n6. **Agent 5 generates** - Ready-to-share social media content\n7. **Report is compiled** - Complete JSON with all outputs\n8. **Results are saved** - climate_fact_check_report.json\n9. **Summary is displayed** - Human-readable overview\n\n**Output you'll see:**\n- Progress indicators for each agent âœ“\n- Summary statistics (true vs false claims)\n- Average credibility score\n- Full breakdown of every fact-check\n- File saved confirmation\n\n**What gets saved:**\n- Complete JSON report with ALL agent outputs\n- Timestamp of generation\n- System configuration used\n- Success/failure metrics\n- Ready for PDF generation or social media sharing\n\nThis is production-grade fact-checking infrastructure. In a few seconds, you'll have transparent, verifiable, detailed fact-checks ready to share. ðŸš€\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# FINAL PIPELINE\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ðŸŽ¯ RUNNING FACT-CHECKING PIPELINE\")\nprint(\"=\" * 80)\nprint(f\"Processing {len(CLAIMS_TO_ANALYZE)} claims through 5-agent system...\")\nprint(\"=\" * 80)\n\n# Create orchestrator and run pipeline\norchestrator = ClimateFactCheckOrchestrator(model, CONFIG)\nfinal_report = orchestrator.execute(CLAIMS_TO_ANALYZE)\n\n# Save and display results\norchestrator.save_report(final_report)\norchestrator.display_summary(final_report)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"âœ… COMPLETE! Check climate_fact_check_report.json for full results\")\nprint(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:42:16.612041Z","iopub.execute_input":"2025-12-01T17:42:16.612388Z","iopub.status.idle":"2025-12-01T17:46:00.720929Z","shell.execute_reply.started":"2025-12-01T17:42:16.612364Z","shell.execute_reply":"2025-12-01T17:46:00.715318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 12: Generate Professional PDF Reports\n\nThe final piece of the puzzle. Your JSON data transforms into a polished, professional PDF report that you can share with stakeholders, publish, or submit.\n\n**What it does:**\n- Loads the JSON report from the orchestrator\n- Creates a beautifully formatted PDF with:\n  - **Title Page** - Report name, date, summary statistics\n  - **Executive Summary** - High-level overview of findings\n  - **Detailed Fact-Checks** - One complete page per claim showing ALL 5 agent outputs\n  - **Professional Styling** - Colors, fonts, spacing optimized for readability\n  - **Page Breaks** - Clean separation between sections\n  - **Full Transparency** - Every agent's work visible and cited\n\n**PDF Contents (per claim):**\n1. ðŸ” Agent 1: Claim Detection (type, confidence, keywords)\n2. âœ“ Agent 2: Source Verification (verdict, consensus, evidence)\n3. ðŸ“š Agent 3: Evidence Synthesis (headline, summary, citations)\n4. â­ Agent 4: Credibility Score (rating, breakdown, justification)\n5. ðŸ“± Agent 5: Counter-Narrative (Twitter, Facebook, Instagram, LinkedIn, TikTok, hashtags, CTA)\n\n**Quality Features:**\n- Clean HTML-escaped text (no rendering errors)\n- Smart formatting of lists and dictionaries\n- Proper citation formatting with sources and URLs\n- Color-coded headers for visual hierarchy\n- Professional color scheme (navy blues, clean whites)\n- Responsive layout that works in any PDF viewer\n\n**Output Files:**\n- `climate_fact_check_report.json` - Raw data, machine-readable\n- `Climate_FactCheck_Report_COMPLETE.pdf` - Presentation-ready, human-readable\n\nThis is publication-quality output suitable for:\n- Academic papers\n- Policy briefs\n- News organizations\n- Educational materials\n- Social media campaigns\n- Government reports\n\nYour entire 5-agent system culminates in this single, professional deliverable. ðŸ“„âœ¨\n","metadata":{}},{"cell_type":"code","source":"from reportlab.lib.pagesizes import letter\nfrom reportlab.lib import colors\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, PageTemplate, Frame\nfrom reportlab.lib.units import inch\nfrom reportlab.lib.enums import TA_JUSTIFY, TA_LEFT, TA_CENTER\nfrom reportlab.pdfgen import canvas\nfrom datetime import datetime\nimport json\n\nclass ProductionPDFGenerator:\n    \"\"\"Production-grade PDF generator with complete outputs and polish\"\"\"\n    \n    def __init__(self, filename='Climate_FactCheck_Report_COMPLETE.pdf'):\n        self.filename = filename\n        self.styles = getSampleStyleSheet()\n        self.setup_custom_styles()\n    \n    def setup_custom_styles(self):\n        \"\"\"Setup custom styles\"\"\"\n        if 'ReportTitle' not in self.styles:\n            self.styles.add(ParagraphStyle(\n                name='ReportTitle',\n                parent=self.styles['Heading1'],\n                fontSize=26,\n                textColor=colors.HexColor('#1F4788'),\n                spaceAfter=20,\n                alignment=TA_CENTER,\n                bold=True\n            ))\n        \n        if 'ClaimHeader' not in self.styles:\n            self.styles.add(ParagraphStyle(\n                name='ClaimHeader',\n                parent=self.styles['Heading2'],\n                fontSize=15,\n                textColor=colors.HexColor('#2A5CAE'),\n                spaceAfter=12,\n                spaceBefore=15,\n                bold=True,\n                borderWidth=1,\n                borderColor=colors.HexColor('#2A5CAE'),\n                borderPadding=5\n            ))\n        \n        if 'AgentHeader' not in self.styles:\n            self.styles.add(ParagraphStyle(\n                name='AgentHeader',\n                parent=self.styles['Heading3'],\n                fontSize=12,\n                textColor=colors.HexColor('#3D5A80'),\n                spaceAfter=6,\n                spaceBefore=8,\n                bold=True,\n                backColor=colors.HexColor('#F0F4F8')\n            ))\n        \n        if 'BodyText' not in self.styles:\n            self.styles.add(ParagraphStyle(\n                name='BodyText',\n                parent=self.styles['BodyText'],\n                fontSize=9,\n                alignment=TA_LEFT,\n                leading=12,\n                spaceBefore=3,\n                spaceAfter=3\n            ))\n    \n    def clean_text(self, text):\n        \"\"\"Clean and escape text for PDF\"\"\"\n        if text is None:\n            return \"Not available\"\n        \n        text_str = str(text)\n        \n        # Handle dict/list formatting\n        if isinstance(text, dict):\n            return self.format_dict(text)\n        if isinstance(text, list):\n            return self.format_list(text)\n        \n        # Escape HTML\n        text_str = text_str.replace('&', '&amp;')\n        text_str = text_str.replace('<', '&lt;')\n        text_str = text_str.replace('>', '&gt;')\n        \n        return text_str\n    \n    def format_dict(self, d):\n        \"\"\"Format dictionary cleanly\"\"\"\n        parts = []\n        for key, value in d.items():\n            parts.append(f\"{key}: {self.clean_text(value)}\")\n        return \"<br/>\".join(parts)\n    \n    def format_list(self, lst):\n        \"\"\"Format list cleanly\"\"\"\n        if not lst:\n            return \"None\"\n        return \"<br/>\".join([f\"â€¢ {self.clean_text(item)}\" for item in lst])\n    \n    def generate_pdf(self, report_data):\n        \"\"\"Generate complete PDF\"\"\"\n        doc = SimpleDocTemplate(\n            self.filename,\n            pagesize=letter,\n            topMargin=0.75*inch,\n            bottomMargin=0.75*inch,\n            leftMargin=0.75*inch,\n            rightMargin=0.75*inch\n        )\n        \n        story = []\n        \n        # Title Page\n        story.extend(self._create_title_page(report_data))\n        story.append(PageBreak())\n        \n        # Executive Summary\n        story.extend(self._create_executive_summary(report_data))\n        story.append(PageBreak())\n        \n        # Process ALL claims\n        for i, fact_check in enumerate(report_data['fact_checks'], 1):\n            story.extend(self._create_complete_claim_section(fact_check, i))\n            story.append(PageBreak())\n        \n        # Build PDF\n        doc.build(story)\n        print(f\"âœ… Production PDF generated: {self.filename}\")\n        print(f\"ðŸ“„ Total claims: {len(report_data['fact_checks'])}\")\n        print(f\"ðŸ“Š Estimated pages: {len(report_data['fact_checks']) * 4 + 2}\")\n    \n    def _create_title_page(self, report_data):\n        \"\"\"Title page\"\"\"\n        story = []\n        \n        title = Paragraph(\"ðŸŒ CLIMATE MISINFORMATION<br/>FACT-CHECK REPORT\", self.styles['ReportTitle'])\n        story.append(title)\n        story.append(Spacer(1, 0.3*inch))\n        \n        timestamp = datetime.fromisoformat(report_data['report_timestamp'])\n        date_str = timestamp.strftime(\"%B %d, %Y at %H:%M UTC\")\n        \n        info_text = f\"\"\"\n        <b>Generated:</b> {date_str}<br/>\n        <b>Total Claims:</b> {report_data['total_claims_analyzed']}<br/>\n        <b>Successfully Analyzed:</b> {report_data['successful_fact_checks']}<br/><br/>\n        \n        <b>ðŸ“Š Summary Statistics:</b><br/>\n        â€¢ True Claims: {report_data['summary']['true_claims']}<br/>\n        â€¢ False Claims: {report_data['summary']['false_claims']}<br/>\n        â€¢ Average Credibility: {report_data['summary']['avg_credibility']:.1f}/100<br/><br/>\n        \n        <b>ðŸ”¬ Sources:</b> {', '.join(report_data['system_config']['scientific_sources'])}\n        \"\"\"\n        \n        story.append(Paragraph(info_text, self.styles['BodyText']))\n        return story\n    \n    def _create_executive_summary(self, report_data):\n        \"\"\"Executive summary\"\"\"\n        story = []\n        \n        title = Paragraph(\"ðŸ“‹ EXECUTIVE SUMMARY\", self.styles['ClaimHeader'])\n        story.append(title)\n        story.append(Spacer(1, 0.15*inch))\n        \n        summary = f\"\"\"\n        This report examined {report_data['total_claims_analyzed']} climate-related claims using a \n        5-agent AI system. Each claim was processed through: Detection, Verification, Synthesis, \n        Credibility Scoring, and Counter-Narrative Generation.<br/><br/>\n        \n        <b>System Configuration:</b><br/>\n        â€¢ Authoritative Sources: {', '.join(report_data['system_config']['scientific_sources'])}<br/>\n        â€¢ Consensus Threshold: {report_data['system_config']['consensus_threshold']}%<br/>\n        â€¢ Success Rate: {report_data['successful_fact_checks']}/{report_data['total_claims_analyzed']} \n        ({100*report_data['successful_fact_checks']/report_data['total_claims_analyzed']:.1f}%)\n        \"\"\"\n        \n        story.append(Paragraph(summary, self.styles['BodyText']))\n        return story\n    \n    def _create_complete_claim_section(self, fact_check, num):\n        \"\"\"Complete claim section - NO TRUNCATION\"\"\"\n        story = []\n        \n        # Claim title\n        title = Paragraph(f\"<b>CLAIM #{num}</b>\", self.styles['ClaimHeader'])\n        story.append(title)\n        story.append(Spacer(1, 0.1*inch))\n        \n        # Original claim\n        claim_text = Paragraph(\n            f\"<b>Original Claim:</b><br/>{self.clean_text(fact_check['original_claim'])}\",\n            self.styles['BodyText']\n        )\n        story.append(claim_text)\n        story.append(Spacer(1, 0.15*inch))\n        \n        # AGENT 1\n        agent1 = fact_check.get('agent1_detection', {})\n        story.append(Paragraph(\"ðŸ” AGENT 1: CLAIM DETECTION\", self.styles['AgentHeader']))\n        \n        a1_text = f\"\"\"\n        <b>Type:</b> {self.clean_text(agent1.get('claim_type'))}<br/>\n        <b>Confidence:</b> {agent1.get('confidence', 0):.2f}<br/>\n        <b>Climate Related:</b> {self.clean_text(agent1.get('is_climate_related'))}<br/>\n        <b>Needs Verification:</b> {self.clean_text(agent1.get('needs_verification'))}<br/>\n        <b>Keywords:</b> {', '.join(agent1.get('keywords', []))}<br/>\n        <b>Reasoning:</b><br/>{self.clean_text(agent1.get('reasoning'))}\n        \"\"\"\n        story.append(Paragraph(a1_text, self.styles['BodyText']))\n        story.append(Spacer(1, 0.1*inch))\n        \n        # AGENT 2\n        agent2 = fact_check.get('agent2_verification', {})\n        story.append(Paragraph(\"âœ“ AGENT 2: SOURCE VERIFICATION\", self.styles['AgentHeader']))\n        \n        a2_text = f\"\"\"\n        <b>Verdict:</b> {self.clean_text(agent2.get('verdict'))}<br/>\n        <b>Scientific Consensus:</b> {agent2.get('scientific_consensus', 0)}%<br/>\n        <b>Confidence:</b> {agent2.get('confidence', 0):.2f}<br/>\n        <b>Sources:</b> {', '.join(agent2.get('authoritative_sources', []))}<br/><br/>\n        \n        <b>Explanation:</b><br/>{self.clean_text(agent2.get('explanation'))}<br/><br/>\n        \n        <b>Supporting Evidence:</b><br/>\n        {self.format_list(agent2.get('supporting_evidence', []))}<br/><br/>\n        \n        <b>Contradicting Evidence:</b><br/>\n        {self.format_list(agent2.get('contradicting_evidence', []))}\n        \"\"\"\n        story.append(Paragraph(a2_text, self.styles['BodyText']))\n        story.append(Spacer(1, 0.1*inch))\n        \n        # AGENT 3\n        agent3 = fact_check.get('agent3_synthesis', {})\n        story.append(Paragraph(\"ðŸ“š AGENT 3: EVIDENCE SYNTHESIS\", self.styles['AgentHeader']))\n        \n        # Format citations properly\n        citations_list = agent3.get('citations', [])\n        citations_formatted = []\n        for idx, citation in enumerate(citations_list, 1):\n            if isinstance(citation, dict):\n                source = citation.get('source', 'Unknown')\n                fact = citation.get('fact', '')\n                url = citation.get('url', '')\n                citations_formatted.append(f\"{idx}. <b>{source}:</b> {self.clean_text(fact)}<br/><i>{url}</i>\")\n            else:\n                citations_formatted.append(f\"{idx}. {self.clean_text(citation)}\")\n        \n        a3_text = f\"\"\"\n        <b>Headline:</b><br/>{self.clean_text(agent3.get('headline'))}<br/><br/>\n        \n        <b>Summary:</b><br/>{self.clean_text(agent3.get('summary'))}<br/><br/>\n        \n        <b>Bottom Line:</b><br/>{self.clean_text(agent3.get('bottom_line'))}<br/><br/>\n        \n        <b>Citations:</b><br/>\n        {'<br/>'.join(citations_formatted)}\n        \"\"\"\n        story.append(Paragraph(a3_text, self.styles['BodyText']))\n        story.append(Spacer(1, 0.1*inch))\n        \n        # AGENT 4\n        agent4 = fact_check.get('agent4_credibility', {})\n        story.append(Paragraph(\"â­ AGENT 4: CREDIBILITY ASSESSMENT\", self.styles['AgentHeader']))\n        \n        a4_text = f\"\"\"\n        <b>Score:</b> {agent4.get('credibility_score', 0)}/100<br/>\n        <b>Rating:</b> {self.clean_text(agent4.get('rating'))}<br/><br/>\n        \n        <b>Breakdown:</b><br/>\n        {self.format_dict(agent4.get('breakdown', {}))}<br/><br/>\n        \n        <b>Justification:</b><br/>\n        {self.clean_text(agent4.get('justification'))}\n        \"\"\"\n        story.append(Paragraph(a4_text, self.styles['BodyText']))\n        story.append(Spacer(1, 0.1*inch))\n        \n        # AGENT 5 - COMPLETE SOCIAL MEDIA\n        agent5 = fact_check.get('agent5_counter_narrative', {})\n        story.append(Paragraph(\"ðŸ“± AGENT 5: COUNTER-NARRATIVE\", self.styles['AgentHeader']))\n        \n        social = agent5.get('social_media', {})\n        \n        a5_text = f\"\"\"\n        <b>Summary:</b><br/>{self.clean_text(agent5.get('short_summary'))}<br/><br/>\n        \n        <b>Twitter (280):</b><br/>{self.clean_text(social.get('twitter_280'))}<br/><br/>\n        \n        <b>Twitter Extended:</b><br/>{self.clean_text(social.get('twitter_extended'))}<br/><br/>\n        \n        <b>Facebook:</b><br/>{self.clean_text(social.get('facebook_long'))}<br/><br/>\n        \n        <b>Instagram:</b><br/>{self.clean_text(social.get('instagram_caption'))}<br/><br/>\n        \n        <b>LinkedIn:</b><br/>{self.clean_text(social.get('linkedin_professional'))}<br/><br/>\n        \n        <b>TikTok:</b><br/>{self.clean_text(social.get('tiktok_script'))}<br/><br/>\n        \n        <b>Hashtags:</b> {' '.join(agent5.get('hashtags', []))}<br/><br/>\n        \n        <b>CTA:</b> {self.clean_text(agent5.get('call_to_action'))}\n        \"\"\"\n        story.append(Paragraph(a5_text, self.styles['BodyText']))\n        \n        return story\n\n\n# ============================================================================\n# EXECUTE PRODUCTION PDF GENERATION\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ðŸ“„ GENERATING PRODUCTION-GRADE PDF (ALL CLAIMS, FULL DETAIL)\")\nprint(\"=\" * 80)\n\n# Load report\nwith open('climate_fact_check_report.json', 'r') as f:\n    report_data = json.load(f)\n\n# Generate\npdf_gen = ProductionPDFGenerator('Climate_FactCheck_Report_COMPLETE.pdf')\npdf_gen.generate_pdf(report_data)\n\nprint(\"\\nâœ… PRODUCTION PDF COMPLETE!\")\nprint(\"ðŸ“„ File: Climate_FactCheck_Report_COMPLETE.pdf\")\nprint(f\"ðŸ“Š All {len(report_data['fact_checks'])} claims included with full agent outputs\")\nprint(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:46:14.349432Z","iopub.execute_input":"2025-12-01T17:46:14.350326Z","iopub.status.idle":"2025-12-01T17:46:14.596380Z","shell.execute_reply.started":"2025-12-01T17:46:14.350292Z","shell.execute_reply":"2025-12-01T17:46:14.595074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CREATE SUBMISSION OUTPUT FILES\n# ============================================================================\n\nimport os\nimport json\nimport csv\nfrom datetime import datetime\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ðŸ“¦ CREATING SUBMISSION OUTPUT FILES\")\nprint(\"=\" * 80)\n\n# Verify final_report exists\nif 'final_report' not in globals():\n    print(\"âŒ ERROR: final_report not found! Run all prior cells first.\")\nelse:\n    try:\n        # 1. Save JSON Report\n        report_json_path = 'climate_fact_check_report.json'\n        with open(report_json_path, 'w') as f:\n            json.dump(final_report, f, indent=2)\n        \n        if os.path.exists(report_json_path):\n            size_kb = os.path.getsize(report_json_path) / 1024\n            print(f\"âœ… JSON Report saved: {report_json_path} ({size_kb:.1f} KB)\")\n        else:\n            print(f\"âŒ JSON save failed!\")\n        \n        # 2. Generate PDF\n        pdf_path = 'Climate_FactCheck_Report_COMPLETE.pdf'\n        pdf_gen = ProductionPDFGenerator(pdf_path)\n        pdf_gen.generate_pdf(final_report)\n        \n        if os.path.exists(pdf_path):\n            size_kb = os.path.getsize(pdf_path) / 1024\n            print(f\"âœ… PDF Report generated: {pdf_path} ({size_kb:.1f} KB)\")\n        else:\n            print(f\"âŒ PDF save failed!\")\n        \n        # 3. Create Summary CSV\n        csv_path = 'climate_fact_check_summary.csv'\n        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n            fieldnames = ['Claim_Number', 'Claim', 'Type', 'Verdict', 'Credibility_Score', 'Rating', 'Confidence']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            \n            writer.writeheader()\n            for idx, fc in enumerate(final_report['fact_checks'], 1):\n                try:\n                    writer.writerow({\n                        'Claim_Number': idx,\n                        'Claim': fc['original_claim'][:100],\n                        'Type': fc['agent1_detection'].get('claim_type', 'N/A'),\n                        'Verdict': fc['agent2_verification'].get('verdict', 'N/A'),\n                        'Credibility_Score': round(fc['agent4_credibility'].get('credibility_score', 0), 1),\n                        'Rating': fc['agent4_credibility'].get('rating', 'N/A'),\n                        'Confidence': round(fc['agent2_verification'].get('confidence', 0), 2)\n                    })\n                except Exception as e:\n                    print(f\"âš ï¸  Skipped row {idx}: {str(e)}\")\n        \n        if os.path.exists(csv_path):\n            size_kb = os.path.getsize(csv_path) / 1024\n            print(f\"âœ… Summary CSV saved: {csv_path} ({size_kb:.1f} KB)\")\n        else:\n            print(f\"âŒ CSV save failed!\")\n        \n        # 4. Create Text Summary Report\n        txt_path = 'climate_fact_check_summary.txt'\n        with open(txt_path, 'w', encoding='utf-8') as f:\n            f.write(\"=\" * 80 + \"\\n\")\n            f.write(\"ðŸŒ CLIMATE FACT-CHECK REPORT SUMMARY\\n\")\n            f.write(\"=\" * 80 + \"\\n\\n\")\n            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n            f.write(f\"Total Claims: {final_report['total_claims_analyzed']}\\n\")\n            f.write(f\"Successfully Processed: {final_report['successful_fact_checks']}\\n\")\n            f.write(f\"True Claims: {final_report['summary']['true_claims']}\\n\")\n            f.write(f\"False Claims: {final_report['summary']['false_claims']}\\n\")\n            f.write(f\"Average Credibility: {final_report['summary']['avg_credibility']:.1f}/100\\n\\n\")\n            \n            for idx, fc in enumerate(final_report['fact_checks'], 1):\n                f.write(f\"\\nCLAIM {idx}:\\n\")\n                f.write(f\"  Original: {fc['original_claim'][:150]}\\n\")\n                f.write(f\"  Verdict: {fc['agent2_verification'].get('verdict', 'N/A')}\\n\")\n                f.write(f\"  Credibility: {fc['agent4_credibility'].get('credibility_score', 'N/A')}/100\\n\")\n                f.write(f\"  Rating: {fc['agent4_credibility'].get('rating', 'N/A')}\\n\")\n        \n        if os.path.exists(txt_path):\n            size_kb = os.path.getsize(txt_path) / 1024\n            print(f\"âœ… Text Summary saved: {txt_path} ({size_kb:.1f} KB)\")\n        else:\n            print(f\"âŒ Text save failed!\")\n        \n        # Final confirmation\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ðŸ“¦ SUBMISSION FILES READY:\")\n        output_files = []\n        for fname in [report_json_path, pdf_path, csv_path, txt_path]:\n            if os.path.exists(fname):\n                size = os.path.getsize(fname)\n                output_files.append(f\"   âœ… {fname} ({size:,} bytes)\")\n        \n        print(\"\\n\".join(output_files))\n        print(f\"\\nðŸŽ¯ Total Output Files: {len(output_files)}/4\")\n        print(\"=\" * 80)\n        \n    except Exception as e:\n        print(f\"\\nâŒ ERROR during file creation: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:46:33.753751Z","iopub.execute_input":"2025-12-01T17:46:33.754040Z","iopub.status.idle":"2025-12-01T17:46:33.932511Z","shell.execute_reply.started":"2025-12-01T17:46:33.754020Z","shell.execute_reply":"2025-12-01T17:46:33.930789Z"}},"outputs":[],"execution_count":null}]}